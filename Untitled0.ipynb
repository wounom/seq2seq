{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8xhJ1ApcXMAy"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义特殊标记\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2  # 用于填充\n",
        "\n",
        "# 定义语言处理类\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\", PAD_token: \"<pad>\"}\n",
        "        self.n_words = 3  # 计数 SOS、EOS 和 PAD\n",
        "\n",
        "    def index_words(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.index_word(word)\n",
        "\n",
        "    def index_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def read_langs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(f'{lang1}-{lang2}.txt', 'r', encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "def filter_pair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filter_pairs(pairs):\n",
        "    return [pair for pair in pairs if filter_pair(pair)]\n",
        "\n",
        "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
        "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "\n",
        "    pairs = filter_pairs(pairs)\n",
        "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "\n",
        "    print(\"Indexing words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.index_words(pair[0])\n",
        "        output_lang.index_words(pair[1])\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "ZoFh6wwUZok3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n"
      ],
      "metadata": {
        "id": "dTtFtTmdZotV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers,batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "# 定义解码器\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers,batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(1)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        #prediction = self.fc_out(output)\n",
        "        return prediction, hidden\n",
        "\n",
        "# 定义seq2seq模型\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len = trg.shape[1]\n",
        "        batch_size = trg.shape[0]\n",
        "        output_dim = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size,trg_len, output_dim).to(self.device)\n",
        "\n",
        "        hidden = self.encoder(src)\n",
        "\n",
        "        # 解码器的输入为目标序列的第一个词\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            outputs[:,t] = output\n",
        "\n",
        "            # 决定是否使用teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[:,t] if teacher_force else output.argmax(1)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "j6C5ZU2BZov-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2  # 用于填充\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "#USE_CUDA = False\n",
        "# 准备数据\n",
        "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
        "input_vocab_size = len(input_lang.index2word)\n",
        "output_vocab_size = len(output_lang.index2word)\n",
        "\n",
        "print(f\"Input vocabulary size: {input_vocab_size}\")\n",
        "print(f\"Output vocabulary size: {output_vocab_size}\")\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 创建训练数据\n",
        "def indexes_from_sentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def variable_from_sentence(lang, sentence):\n",
        "    indexes = indexes_from_sentence(lang, sentence)\n",
        "    indexes.append(EOS_token)  # 添加 EOS_token\n",
        "    var = torch.LongTensor(indexes)\n",
        "    if USE_CUDA:\n",
        "        var = var.cuda()  # 转移到 GPU\n",
        "    return var\n",
        "\n",
        "def variables_from_pair(pair):\n",
        "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
        "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
        "    return (input_variable, target_variable)\n",
        "\n",
        "train_data = [variables_from_pair(pair) for pair in pairs]\n",
        "input_variables, target_variables = zip(*train_data)\n",
        "\n",
        "# 使用 pad_sequence 填充输入和目标张量\n",
        "input_tensor = pad_sequence(input_variables, batch_first=True, padding_value=PAD_token )  # [batch_size ,max_length]\n",
        "target_tensor = pad_sequence(target_variables, batch_first=True, padding_value=PAD_token )  #[batch_size ,max_length]\n",
        "\n",
        "# 创建 DataLoader\n",
        "dataset = TensorDataset(input_tensor, target_tensor)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 模型参数\n",
        "ENC_EMB_DIM = 300\n",
        "DEC_EMB_DIM = 300\n",
        "HID_DIM = 256\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# 初始化模型\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "enc = Encoder(len(input_lang.index2word), ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "dec = Decoder(len(output_lang.index2word), DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# 训练设置\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# 训练函数\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in iterator:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        intrg=trg[:,:-1]\n",
        "        # 创建一个全0的Tensor，其形状与第二维的每个子Tensor相同，除了第一个维度是1\n",
        "        zero_tensor = torch.zeros((intrg.size(0), 1)).long().to(device)\n",
        "\n",
        "        # 使用torch.cat在第二维（dim=1）上拼接这两个Tensor\n",
        "        resultin_trg = torch.cat((zero_tensor, intrg), dim=1)\n",
        "\n",
        "        output = model(src, resultin_trg)#[batch_size,sequence_len,output_dim]\n",
        "        logp = nn.functional.log_softmax(output, dim=-1)\n",
        "\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        logp = logp.reshape(-1, output_dim)  # 使用 reshape\n",
        "        trg = trg.reshape(-1)  # 使用 reshape\n",
        "\n",
        "        loss = criterion(logp, trg)  # 计算损失\n",
        "        #print(loss)\n",
        "        loss.backward()\n",
        "       # print(output.size())\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # 梯度裁剪\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# 开始训练\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    start = datetime.datetime.now()\n",
        "    loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    end = datetime.datetime.now()\n",
        "    print(f'Epoch {epoch}, Loss: {loss:.4f},用时:{end-start}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeEP7nExZoyj",
        "outputId": "6e989fc4-cf08-423c-c26a-09259a6c33fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 20000 sentence pairs\n",
            "Trimmed to 19920 sentence pairs\n",
            "Indexing words...\n",
            "Input vocabulary size: 6069\n",
            "Output vocabulary size: 3439\n",
            "Epoch 1, Loss: 3.2278,用时:0:00:11.880610\n",
            "Epoch 2, Loss: 2.6748,用时:0:00:14.263614\n",
            "Epoch 3, Loss: 2.4429,用时:0:00:11.574438\n",
            "Epoch 4, Loss: 2.2787,用时:0:00:09.636923\n",
            "Epoch 5, Loss: 2.1500,用时:0:00:09.511301\n",
            "Epoch 6, Loss: 2.0473,用时:0:00:09.547615\n",
            "Epoch 7, Loss: 1.9608,用时:0:00:09.525235\n",
            "Epoch 8, Loss: 1.8867,用时:0:00:09.549306\n",
            "Epoch 9, Loss: 1.8225,用时:0:00:09.228362\n",
            "Epoch 10, Loss: 1.7727,用时:0:00:09.446836\n"
          ]
        }
      ]
    }
  ]
}